\section{Evolución temporal de métodos de detección de anomalías en series temporales}
En este capítulo se presenta un análisis integral de la progresión histórica de las metodologías de detección de anomalías en series temporales, abarcando casi un siglo de avance tecnológico desde el control estadístico de procesos fundamental hasta las redes neuronales de impulsos. La evolución revela paradigmas tecnológicos distintos que han dado forma al campo, cada uno abordando limitaciones específicas mientras introduce nuevas capacidades en términos de eficiencia computacional, precisión de detección y aplicabilidad en el mundo real. A través del examen sistemático de innovaciones, este estudio identifica hitos tecnológicos clave y proporciona una verificación crítica de los marcos cronológicos existentes. El análisis demuestra cómo las compensaciones entre eficiencia energética y precisión han impulsado transiciones metodológicas, al tiempo que destaca enfoques híbridos emergentes que combinan los beneficios de múltiples paradigmas para mejorar el rendimiento en la detección de anomalías.

\section{Fundamentos estadísticos y enfoques clásicos}
\subsection{Métodos Iniciales de Control Estadístico de Procesos}

El origen de la detección sistemática de anomalías en series temporales se remonta al trabajo pionero de Walter Shewhart en 1924, quien introdujo los gráficos de control estadístico (\textbf{SPC}) \cite{shewhart_economic_1931}. Estos gráficos proporcionaron el primer marco formal para distinguir entre variación común y variación especial en procesos industriales, cambiando fundamentalmente cómo las organizaciones abordan la supervisión de la calidad y la identificación de anomalías. El enfoque de Shewhart enfatizó la importancia de comprender la variación natural del proceso antes de intentar identificar verdaderas anomalías, un principio que sigue siendo central en los sistemas modernos de detección de anomalías.

La evolución de los métodos de control estadístico continuó con la introducción del gráfico de suma acumulada (\textbf{CUSUM}) por Page en 1954, un avance significativo en la sensibilidad para detectar pequeños cambios en el comportamiento de los procesos \cite{page_continuous_1954}. Los gráficos CUSUM acumulan información de observaciones anteriores, demostrando ser superiores a los gráficos Shewhart tradicionales para detectar desviaciones graduales \cite{gualandi_worst-case_2023,gualandi_optimization-based_2022}. Este enfoque acumulativo resultó particularmente valioso para escenarios donde las anomalías se manifiestan como desviaciones sutiles y persistentes en lugar de picos dramáticos, haciendo de CUSUM una herramienta esencial para los sistemas ciberfísicos modernos y aplicaciones de seguridad.


\subsection{Técnicas avanzadas de descomposición estadística}

En 1967, MacQueen  propone un método llamado  agrupamiento \textbf{K-means}. Este enfoque basado en agrupamiento para la detección de anomalías representa un hito importante en los métodos no supervisados, donde las anomalías se identifican como puntos que no pertenecen a ningún grupo bien definido o forman grupos muy pequeños y aislados \cite{morissette_k-means_2013}.

La década de 1990 marcó un avance significativo con el desarrollo del procedimiento de descomposición estacional-tendencia basado en pérdida (\textbf{STL}), que permitió separar series temporales en componentes de tendencia, estacionalidad y residuos, facilitando la detección de anomalías con mayor precisión al modelar patrones esperados e identificar desviaciones de estos patrones con mayor precisión \cite{cleveland_stl_1990}. El método demostró ser particularmente efectivo para aplicaciones de vigilancia en salud, donde modeló con éxito recuentos diarios de quejas principales descomponiéndolos en componentes interanuales, estacionales anuales, día de la semana y errores aleatorios \cite{hafen_syndromic_2009}. 
